{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# Modeling - Life Insurance Subscription Prediction\n", "\n", "INSAT GL4 Mini-Project - 5 ML Models with CV"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\nfrom imblearn.over_sampling import SMOTE\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('Libraries loaded!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Data Loading"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('../data/train.csv')\nprint(f'Shape: {df.shape}')\ndf.head()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Preprocessing"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = df.drop('id', axis=1)\nQ1 = df['Annual_Premium'].quantile(0.25)\nQ3 = df['Annual_Premium'].quantile(0.75)\nIQR = Q3 - Q1\ndf['Annual_Premium'] = df['Annual_Premium'].clip(lower=Q1-1.5*IQR, upper=Q3+1.5*IQR)\nprint('Outliers handled')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["le = LabelEncoder()\ndf['Gender'] = le.fit_transform(df['Gender'])\ndf['Vehicle_Age'] = df['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})\ndf['Vehicle_Damage'] = df['Vehicle_Damage'].map({'Yes': 1, 'No': 0})\nprint('Encoding done')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = df.drop('Response', axis=1)\ny = df['Response']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nprint(f'Train: {len(X_train)}, Test: {len(X_test)}')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_cols = ['Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage']\nscaler = StandardScaler()\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_test[num_cols] = scaler.transform(X_test[num_cols])\nprint('Scaling done')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["smote = SMOTE(random_state=42)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\nprint(f'After SMOTE: {len(X_train_bal)}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Define Models"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["models = {\n    'Logistic Regression': LogisticRegression(C=1.0, solver='liblinear', random_state=42),\n    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n    'KNN': KNeighborsClassifier(n_neighbors=5, metric='euclidean', n_jobs=-1),\n    'XGBoost': XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, eval_metric='logloss'),\n    'SVM': SVC(kernel='rbf', C=1.0, probability=True, random_state=42)\n}\nprint(f'{len(models)} models defined')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Cross-Validation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nresults = {'Model': [], 'CV Accuracy': [], 'CV F1': [], 'CV ROC-AUC': []}\n\nsample = min(50000, len(X_train_bal))\nnp.random.seed(42)\nidx = np.random.choice(len(X_train_bal), sample, replace=False)\nX_cv = X_train_bal.iloc[idx]\ny_cv = y_train_bal.iloc[idx]\n\nfor name, model in models.items():\n    print(f'CV: {name}...')\n    acc = cross_val_score(model, X_cv, y_cv, cv=cv, scoring='accuracy', n_jobs=-1).mean()\n    f1 = cross_val_score(model, X_cv, y_cv, cv=cv, scoring='f1', n_jobs=-1).mean()\n    auc = cross_val_score(model, X_cv, y_cv, cv=cv, scoring='roc_auc', n_jobs=-1).mean()\n    results['Model'].append(name)\n    results['CV Accuracy'].append(acc)\n    results['CV F1'].append(f1)\n    results['CV ROC-AUC'].append(auc)\n    print(f'  Acc: {acc:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}')"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_df = pd.DataFrame(results).sort_values('CV ROC-AUC', ascending=False)\nprint(results_df.to_string(index=False))"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor i, m in enumerate(['CV Accuracy', 'CV F1', 'CV ROC-AUC']):\n    axes[i].bar(results_df['Model'], results_df[m])\n    axes[i].set_title(m)\n    axes[i].tick_params(axis='x', rotation=45)\nplt.tight_layout()\nplt.savefig('../figs/cv_comparison.png', dpi=150)\nplt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Train Models"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["trained = {}\nfor name, model in models.items():\n    print(f'Training {name}...')\n    model.fit(X_train_bal, y_train_bal)\n    trained[name] = model\nprint('All models trained!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Test Evaluation"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["test_res = {'Model': [], 'Test Acc': [], 'Test F1': [], 'Test AUC': []}\nfor name, model in trained.items():\n    y_pred = model.predict(X_test)\n    y_proba = model.predict_proba(X_test)[:, 1]\n    test_res['Model'].append(name)\n    test_res['Test Acc'].append(accuracy_score(y_test, y_pred))\n    test_res['Test F1'].append(f1_score(y_test, y_pred))\n    test_res['Test AUC'].append(roc_auc_score(y_test, y_proba))\ntest_df = pd.DataFrame(test_res).sort_values('Test AUC', ascending=False)\nprint(test_df.to_string(index=False))"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 7. ROC Curves"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\nfor name, model in trained.items():\n    y_proba = model.predict_proba(X_test)[:, 1]\n    fpr, tpr, _ = roc_curve(y_test, y_proba)\n    auc = roc_auc_score(y_test, y_proba)\n    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC={auc:.4f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC Curves')\nplt.legend(loc='lower right')\nplt.tight_layout()\nplt.savefig('../figs/roc_curves.png', dpi=150)\nplt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Confusion Matrices"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\nfor i, (name, model) in enumerate(trained.items()):\n    y_pred = model.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n    axes[i].set_title(name)\naxes[5].axis('off')\nplt.tight_layout()\nplt.savefig('../figs/confusion_matrices.png', dpi=150)\nplt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 9. GridSearchCV (Top 2)"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["top2 = results_df.head(2)['Model'].tolist()\nprint(f'Tuning: {top2}')\n\nparams = {\n    'Random Forest': {'n_estimators': [50, 100], 'max_depth': [5, 10, None]},\n    'XGBoost': {'n_estimators': [50, 100], 'max_depth': [3, 5], 'learning_rate': [0.1, 0.2]},\n    'Logistic Regression': {'C': [0.1, 1, 10]},\n    'KNN': {'n_neighbors': [3, 5, 7]},\n    'SVM': {'C': [0.1, 1, 10]}\n}\n\nsample_grid = min(20000, len(X_train_bal))\nidx_grid = np.random.choice(len(X_train_bal), sample_grid, replace=False)\nX_grid = X_train_bal.iloc[idx_grid]\ny_grid = y_train_bal.iloc[idx_grid]"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["best_models = {}\nfor name in top2:\n    print(f'Tuning {name}...')\n    if name == 'Random Forest':\n        m = RandomForestClassifier(random_state=42, n_jobs=-1)\n    elif name == 'XGBoost':\n        m = XGBClassifier(random_state=42, eval_metric='logloss')\n    elif name == 'Logistic Regression':\n        m = LogisticRegression(random_state=42)\n    elif name == 'KNN':\n        m = KNeighborsClassifier(n_jobs=-1)\n    else:\n        m = SVC(probability=True, random_state=42)\n    gs = GridSearchCV(m, params[name], cv=3, scoring='roc_auc', n_jobs=-1)\n    gs.fit(X_grid, y_grid)\n    best_models[name] = gs.best_estimator_\n    print(f'  Best: {gs.best_params_}, AUC: {gs.best_score_:.4f}')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 10. Feature Importance"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["feat_names = X.columns.tolist()\nrf = trained.get('Random Forest', trained.get('XGBoost'))\nimp = rf.feature_importances_\nidx = np.argsort(imp)[::-1]\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(imp)), imp[idx])\nplt.xticks(range(len(imp)), [feat_names[i] for i in idx], rotation=45, ha='right')\nplt.title('Feature Importance')\nplt.tight_layout()\nplt.savefig('../figs/feature_importance.png', dpi=150)\nplt.show()"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 11. Classification Report"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["best_name = test_df.iloc[0]['Model']\nbest = trained[best_name]\ny_pred = best.predict(X_test)\nprint(f'Best Model: {best_name}')\nprint(classification_report(y_test, y_pred, target_names=['No', 'Yes']))"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 12. Save Models"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["joblib.dump(best, '../models/best_model.pkl')\njoblib.dump(scaler, '../models/scaler.pkl')\njoblib.dump(le, '../models/label_encoder.pkl')\nfor name, model in trained.items():\n    joblib.dump(model, f'../models/{name.lower().replace(\" \", \"_\")}.pkl')\nprint('All models saved!')"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 13. Summary"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print('='*50)\nprint('MODELING SUMMARY')\nprint('='*50)\nprint(f'Train samples: {len(X_train_bal):,}')\nprint(f'Test samples: {len(X_test):,}')\nprint('\\nCV Results:')\nprint(results_df.to_string(index=False))\nprint('\\nTest Results:')\nprint(test_df.to_string(index=False))\nprint(f'\\nBest Model: {best_name}')\nprint('\\nModeling complete!')"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.0"}},
 "nbformat": 4,
 "nbformat_minor": 4
}
